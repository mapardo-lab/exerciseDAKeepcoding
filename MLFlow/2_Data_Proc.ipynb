{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7liv6haBNZb"
   },
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm2NRDIAqEzI"
   },
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "T4InpvkuquUm",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: numpy in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.10.3)\n",
      "Requirement already satisfied: seaborn in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (1.7.0)\n",
      "Requirement already satisfied: num2words in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.5.14)\n",
      "Requirement already satisfied: gensim in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (4.3.3)\n",
      "Requirement already satisfied: wordcloud in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.9.4)\n",
      "Requirement already satisfied: nltk in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (3.9.1)\n",
      "Requirement already satisfied: joblib in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from num2words->-r requirements.txt (line 6)) (0.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from gensim->-r requirements.txt (line 7)) (7.3.0)\n",
      "Requirement already satisfied: click in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 9)) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 9)) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 9)) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: wrapt in /home/mapardo/miniconda3/envs/bd15-despliegue-algo/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim->-r requirements.txt (line 7)) (1.17.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NSKbPus5P6b0"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import unicodedata\n",
    "from num2words import num2words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1750783198546,
     "user": {
      "displayName": "Miguel Angel Pardo",
      "userId": "16900940354237523056"
     },
     "user_tz": -120
    },
    "id": "Pyct7ps9okI1",
    "outputId": "0cf64f00-795b-490b-86f7-c1d93ed6151f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13272, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "data = pd.read_json('reviews_Patio_Lawn_and_Garden_5.json', lines=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8j_SwxwopG1s"
   },
   "outputs": [],
   "source": [
    "# Split into train and test datasets\n",
    "reviewText_train, reviewText_test, overall_train, overall_test = train_test_split(\n",
    "    data['reviewText'], data['overall'],\n",
    "    train_size=0.75, test_size=0.25,\n",
    "    random_state=42, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qAikA_eBB-u_"
   },
   "outputs": [],
   "source": [
    "def normalize_ASCII(text):\n",
    "  \"\"\"\n",
    "  Normalizes Unicode text to its ASCII representation by decomposing complex characters,\n",
    "  removing non-ASCII characters, and returning a clean UTF-8 string.\n",
    "  \"\"\"\n",
    "  text = unicodedata.normalize('NFKD', text) # Break down Unicode Characters\n",
    "  text = text.encode('ascii', 'ignore') # Convert to ASCII\n",
    "  text = text.decode('utf-8', 'ignore') # Decode back to UTF-8\n",
    "  return text\n",
    "\n",
    "def cleanning(text):\n",
    "  \"\"\"\n",
    "  Cleans text by:\n",
    "  1. Removing all punctuation except apostrophes (for contractions)\n",
    "  2. Eliminating single-letter words\n",
    "  \"\"\"\n",
    "  text = re.sub(r\"[^a-z0-9']\", ' ', text) # Remove punctuation. Only words, spaces and ' are kept. Keep ' is important for removing stopwords step.\n",
    "  text = re.sub(r\" [a-z] \", ' ', text) # Remove one letter words\n",
    "  return text\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "  \"\"\"\n",
    "  Removes stopwords and apostrophes from input text, preserving meaningful words.\n",
    "  \"\"\"\n",
    "  list_words = []\n",
    "  for word in text.split():\n",
    "    if word not in stopwords: # Remove stopwords\n",
    "      list_words.append(word)\n",
    "  cleaned_text = ' '.join(list_words)\n",
    "  cleaned_text = re.sub(r\"'\", '', cleaned_text) # Remove punctuation '\n",
    "  return cleaned_text\n",
    "\n",
    "def numbers2words(text):\n",
    "  \"\"\"\n",
    "  Converts all numeric digits in a text string to their word equivalents\n",
    "  \"\"\"\n",
    "  list_words = []\n",
    "  for word in text.split():\n",
    "    if word.isdigit():\n",
    "      list_words.append(num2words(word, ordinal=False))\n",
    "    else:\n",
    "      list_words.append(word)\n",
    "  new_text = ' '.join(list_words)\n",
    "  return new_text\n",
    "\n",
    "def stemming(text):\n",
    "  \"\"\"\n",
    "  Applies Porter stemming to each word in the input text, reducing words to their root forms.\n",
    "  \"\"\"\n",
    "  stemmer = PorterStemmer()\n",
    "  list_words = []\n",
    "  for word in text.split():\n",
    "    list_words.append(PorterStemmer().stem(word))\n",
    "  return ' '.join(list_words)\n",
    "\n",
    "def overall2label(overall):\n",
    "  \"\"\"\n",
    "  Converts a numerical 'overall' rating into a binary label:\n",
    "  - 0 for ratings below 4 (negative)\n",
    "  - 1 for ratings 4 or above (positive)\n",
    "  \"\"\"\n",
    "  label = None\n",
    "  if overall < 4:\n",
    "    label = 0\n",
    "  else:\n",
    "    label = 1\n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JHTY1uLBSRNS"
   },
   "outputs": [],
   "source": [
    "def review2words(text):\n",
    "  \"\"\"\n",
    "  Applies a complete text preprocessing pipeline to normalize and clean input text.\n",
    "  Performs the following transformations in sequence:\n",
    "  1. Converts to lowercase\n",
    "  2. Normalizes Unicode to ASCII\n",
    "  3. Cleans punctuation and single-letter words\n",
    "  4. Removes stopwords\n",
    "  5. Applies stemming\n",
    "  6. Converts numbers to words\n",
    "  \"\"\"\n",
    "  text = text.lower() # To lowercase\n",
    "  text = normalize_ASCII(text)\n",
    "  text = cleanning(text)\n",
    "  text = remove_stopwords(text, STOPWORDS)\n",
    "  text = stemming(text)\n",
    "  text = numbers2words(text)\n",
    "  words = text.split()\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1750783229219,
     "user": {
      "displayName": "Miguel Angel Pardo",
      "userId": "16900940354237523056"
     },
     "user_tz": -120
    },
    "id": "AIU_FBd3DELD",
    "outputId": "f4f593b6-ea1f-407c-9d2f-ed2b9ef543f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "This chain fit great for my Poulan Pro electric saw. This is also an older company that has a great website to help identify exactly what model your specific saw requires. It is not directional, so you don't have to worry about any arrows facing the right way. I just used it and I forgot how powerful my saw was. Getting old ones sharpened can be questionable when new ones are this cheap.\n",
      "After:\n",
      "['chain', 'fit', 'great', 'poulan', 'pro', 'electr', 'saw', 'older', 'compani', 'great', 'websit', 'help', 'identifi', 'exactli', 'model', 'specif', 'saw', 'requir', 'direct', 'worri', 'arrow', 'face', 'right', 'way', 'use', 'forgot', 'power', 'saw', 'get', 'old', 'one', 'sharpen', 'question', 'new', 'one', 'cheap']\n"
     ]
    }
   ],
   "source": [
    "# Check processing\n",
    "print(f'Before:\\n{data.loc[201,\"reviewText\"]}')\n",
    "print(f'After:\\n{review2words(data.loc[201,\"reviewText\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jzSvD7b4OoIp"
   },
   "outputs": [],
   "source": [
    "cache_dir = \"cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"\n",
    "    Preprocesses training and test data by:\n",
    "    1. Converting reviews to tokenized words\n",
    "    2. Transforming ratings to binary labels\n",
    "    3. Caching/loading processed data for efficiency\n",
    "    \"\"\"\n",
    "\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if cache_data is None:\n",
    "        words_train = list(map(review2words, data_train))\n",
    "        words_test = list(map(review2words, data_test))\n",
    "        labels_train = list(map(overall2label, labels_train))\n",
    "        labels_test = list(map(overall2label, labels_test))\n",
    "\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "\n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41882,
     "status": "ok",
     "timestamp": 1750783277192,
     "user": {
      "displayName": "Miguel Angel Pardo",
      "userId": "16900940354237523056"
     },
     "user_tz": -120
    },
    "id": "MVJ6xYMBmDEs",
    "outputId": "05a34a01-17dc-4976-80d7-14cd1ecc9ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "words_train, words_test, labels_train, labels_test = preprocess_data(reviewText_train, reviewText_test, overall_train, overall_test, cache_dir)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM3d13C1UDDt6mmTKvyCtLJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
